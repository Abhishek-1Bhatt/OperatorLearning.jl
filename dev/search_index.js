var documenterSearchIndex = {"docs":
[{"location":"reference/","page":"Module Reference","title":"Module Reference","text":"Modules = [OperatorLearning]","category":"page"},{"location":"reference/#OperatorLearning.FourierLayer","page":"Module Reference","title":"OperatorLearning.FourierLayer","text":"FourierLayer(in, out, batch, grid, modes, σ=identity, init=glorotuniform) FourierLayer(Wf::AbstractArray, Wl::AbstractArray, [biasf, bias_l, σ])\n\nCreate a Layer of the Fourier Neural Operator as proposed by Zongyi et al. arXiv: 2010.08895\n\nThe layer does a fourier transform on the last axis (the coeffs) of the input array, filters higher modes out by the weight matrix and transforms the second axis to the specified output dimension such that In x M x N -> Out x M x N. The output though only contains the relevant Fourier modes with the rest padded to zero in the last axis as a result of the filtering.\n\nThe input x should be a rank 3 tensor of shape (num parameters (in) x num grid points (grid) x batch size (batch)) The output y will be a rank 3 tensor of shape (out x num grid points (grid) x batch size (batch))\n\nYou can specify biases for the paths as you like, though the convolutional path is originally not intended to perform an affine transformation.\n\nExamples\n\nSay you're considering a 1D diffusion problem on a 64 point grid. The input is comprised of the grid points as well as the IC at this point. The data consists of 200 instances of the solution. Beforehand we convert the inputs into a higher-dimensional latent space with 128 nodes by using a regular Dense layer. So the input takes the dimension 128 x 64 x 200. The output would be the diffused variable at a later time, which initially makes the output of the form 128 x 64 x 200 as well. Finally, we have to squeeze this high-dimensional ouptut into the one quantity of interest again by using a Dense layer.\n\nWe wish to only keep the first 16 modes of the input and work with the classic sigmoid function as activation.\n\nSo we would have:\n\nmodel = FourierLayer(128, 128, 200, 100, 16, σ)\n\n\n\n\n\n","category":"type"},{"location":"reference/#OperatorLearning.cglorot_normal-Tuple{Random.AbstractRNG, Vararg{Any}}","page":"Module Reference","title":"OperatorLearning.cglorot_normal","text":"cglorotnormal([rng=GLOBALRNG], dims...)\n\nA modification of the glorot_normal function provided by Flux to accommodate Complex numbers. This is necessary since the parameters of the global convolution operator in the Fourier Layer generally has complex weights.\n\n\n\n\n\n","category":"method"},{"location":"reference/#OperatorLearning.cglorot_uniform-Tuple{Random.AbstractRNG, Vararg{Any}}","page":"Module Reference","title":"OperatorLearning.cglorot_uniform","text":"cglorotuniform([rng=GLOBALRNG], dims...)\n\nA modification of the glorot_uniform function provided by Flux to accommodate Complex numbers. This is necessary since the parameters of the global convolution operator in the Fourier Layer generally has complex weights.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = OperatorLearning","category":"page"},{"location":"#OperatorLearning","page":"Home","title":"OperatorLearning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for OperatorLearning.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Simply install by running in a REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add OperatorLearning","category":"page"},{"location":"#Usage/Examples","page":"Home","title":"Usage/Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The basic workflow is more or less in line with the layer architectures that Flux provides, i.e. you construct individual layers, chain them if desired and pass the inputs as arguments to the layers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Fourier Layer performs a linear transform as well as convolution (linear transform in fourier space), adds them and passes it through the activation. Additionally, higher Fourier modes are filtered out in the convolution path where you can specify the amount of modes to be kept.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The syntax for a single Fourier Layer is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using OperatorLearning\nusing Flux\n\n# Input = 101, Output = 101, Batch size = 200, Grid points = 100, Fourier modes = 16\n# Activation: sigmoid (you need to import Flux in your Script to access the activations)\nmodel = FourierLayer(101, 101, 200, 100, 16, σ)\n\n# Same as above, but perform strict convolution in Fourier Space\nmodel = FourierLayer(101, 101, 200, 100, 16, σ; bias_fourier=false)","category":"page"},{"location":"","page":"Home","title":"Home","text":"To see a full implementation, check the Burgers equation example at examples/burgers.jl.","category":"page"}]
}
