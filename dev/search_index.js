var documenterSearchIndex = {"docs":
[{"location":"examples/burgers_DeepONet/#Solving-the-Burgers-Equation-with-DeepONet","page":"Burgers Equation with DeepONet","title":"Solving the Burgers Equation with DeepONet","text":"","category":"section"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"This example mostly adapts the original work by Li et al to solving with DeepONet and is intended to provide an analogue to the FNO example.","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"We try to create an operator for the Burgers equation","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"partial_t u(xt) + partial_x (u^2(xt)2) = nu partial_xx u(xt)","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"in one dimension for a unit spacial and temporal domain. The operator maps the initial condition u(x0) = u_0(x) to the flow field at the final time u(x1).","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"So overall, we need an approximation function that does the following:","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"function foo(u0,x)\n    # Do something\n    return u1","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"We sample from a dataset that contains several instances of the initial condition (a) and the final velocity field (u). The data is given on a grid of 8192 points, however we would like to only sample 1024 points.","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"using Flux: length, reshape, train!, throttle, @epochs\nusing OperatorLearning, Flux, MAT\n\ndevice = cpu;\n\n#=\nWe would like to implement and train a DeepONet that infers the solution\nu(x) of the burgers equation on a grid of 1024 points at time one based\non the initial condition a(x) = u(x,0)\n=#\n\n# Read the data from MAT file and store it in a dict\n# key \"a\" is the IC\n# key \"u\" is the desired solution at time 1\nvars = matread(\"burgers_data_R10.mat\") |> device\n\n# For trial purposes, we might want to train with different resolutions\n# So we sample only every n-th element\nsubsample = 2^3;\n\n# create the x training array, according to our desired grid size\nxtrain = vars[\"a\"][1:1000, 1:subsample:end]' |> device;\n# create the x test array\nxtest = vars[\"a\"][end-99:end, 1:subsample:end]' |> device;\n\n# Create the y training array\nytrain = vars[\"u\"][1:1000, 1:subsample:end] |> device;\n# Create the y test array\nytest = vars[\"u\"][end-99:end, 1:subsample:end] |> device;","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"One particular thing to note here is that we need to permute the array containing the initial condition so that the inner product of DeepONet works. This is because we need to do the following contraction:","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"sumlimits_i t_ji b_ik = u_jk","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"For now, we only have one input and one output array. In addition, we need another input array that provides the probing locations for the operator u_1(x) = mathcalG(u_0)(x). In theory, we could choose those arbitrarily. For sake of simplicity though, we simply create the same equispaced grid that the original data was sampled from, i.e. a 1-D grid of 1024 equispaced points in [0;1]. Again, we need to transpose the array so that the array dim that is transformed by the trunk network is in the first column - otherwise the inner product would be much more cumbersome to handle.","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"# The data is missing grid data, so we create it\n# `collect` converts data type `range` into an array\ngrid = collect(range(0, 1, length=1024))' |> device","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"We can now set up the DeepONet. We choose the latent space to have dimensionality 1024 and use the vanilla DeepONet architecture, i.e. we use Dense layers in both branch and trunk net. Both contain two layers and use the GeLU activation function:","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"# Create the DeepONet:\n# IC is given on grid of 1024 points, and we solve for a fixed time t in one\n# spatial dimension x, making the branch input of size 1024 and trunk size 1\n# We choose GeLU activation for both subnets\nmodel = DeepONet((1024,1024,1024),(1,1024,1024),gelu,gelu) |> device","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"The rest is more or less boilerplate training code for a DNN, with one exception: For the loss to compute properly, we need to pass two separate input arrays for the branch and trunk network each. We employ the ADAM optimizer with a fixed learning rate of 1e-3, use the mean squared error as loss, evaluate the test loss as callback and train the FNO for 500 epochs.","category":"page"},{"location":"examples/burgers_DeepONet/","page":"Burgers Equation with DeepONet","title":"Burgers Equation with DeepONet","text":"# We use the ADAM optimizer for training\nlearning_rate = 0.001\nopt = ADAM(learning_rate)\n\n# Specify the model parameters\nparameters = params(model)\n\n# The loss function\n# We can't use the \"vanilla\" implementation of the mse here since we have\n# two distinct inputs to our DeepONet, so we wrap them into a tuple\nloss(xtrain,ytrain,sensor) = Flux.Losses.mse(model(xtrain,sensor),ytrain)\n\n# Define a callback function that gives some output during training\nevalcb() = @show(loss(xtest,ytest,grid))\n# Print the callback only every 5 seconds\nthrottled_cb = throttle(evalcb, 5)\n\n# Do the training loop\nFlux.@epochs 500 train!(loss, parameters, [(xtrain,ytrain,grid)], opt, cb = evalcb)","category":"page"},{"location":"examples/burgers_FNO/#Solving-the-Burgers-Equation-with-the-Fourier-Neural-Operator","page":"Burgers Equation with FNO","title":"Solving the Burgers Equation with the Fourier Neural Operator","text":"","category":"section"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"This example mostly replicates the original work by Li et al.","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"We try to create an operator for the Burgers equation","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"partial_t u(xt) + partial_x (u^2(xt)2) = nu partial_xx u(xt)","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"in one dimension for a unit spacial and temporal domain. The operator maps the initial condition u(x0) = u_0(x) to the flow field at the final time u(x1).","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"So overall, we need an approximation function that does the following:","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"function foo(u0,x)\n    # Do something\n    return u1","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"We sample from a dataset that contains several instances of the initial condition (a) and the final velocity field (u). The data is given on a grid of 8192 points, however we would like to only sample 1024 points.","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"using Flux: length, reshape, train!, throttle, @epochs\nusing OperatorLearning, Flux, MAT\n\ndevice = gpu;\n\n# Read the data from MAT file and store it in a dict\nvars = matread(\"burgers_data_R10.mat\") |> device\n\n# For trial purposes, we might want to train with different resolutions\n# So we sample only every n-th element\nsubsample = 2^3;\n\n# create the x training array, according to our desired grid size\nxtrain = vars[\"a\"][1:1000, 1:subsample:end] |> device;\n# create the x test array\nxtest = vars[\"a\"][end-99:end, 1:subsample:end] |> device;\n\n# Create the y training array\nytrain = vars[\"u\"][1:1000, 1:subsample:end] |> device;\n# Create the y test array\nytest = vars[\"u\"][end-99:end, 1:subsample:end] |> device;","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"For now, we only have one input and one output array. In addition, we need corresponding x values for a(x) and u(x) as the second input array which at this point are still missing. The data were sampled from an equispaced grid (otherwise the FFT in our architecture wouldn't work anyway), so manually creating them is fairly straightforward:","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"# The data is missing grid data, so we create it\n# `collect` converts data type `range` into an array\ngrid = collect(range(0, 1, length=length(xtrain[1,:]))) |> device\n\n# Merge the created grid with the data\n# Output has the dims: batch x grid points x 2  (a(x), x)\n# First, reshape the data to a 3D tensor,\n# Then, create a 3D tensor from the synthetic grid data\n# and concatenate them along the newly created 3rd dim\nxtrain = cat(reshape(xtrain,(1000,1024,1)),\n            reshape(repeat(grid,1000),(1000,1024,1));\n            dims=3) |> device\nytrain = cat(reshape(ytrain,(1000,1024,1)),\n            reshape(repeat(grid,1000),(1000,1024,1));\n            dims=3) |> device\n# Same treatment with the test data\nxtest = cat(reshape(xtest,(100,1024,1)),\n            reshape(repeat(grid,100),(100,1024,1));\n            dims=3) |> device\nytest = cat(reshape(ytest,(100,1024,1)),\n            reshape(repeat(grid,100),(100,1024,1));\n            dims=3) |> device","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"Next we need to consider the shape that the FourierLayer expects the inputs to be, i.e. [numInputs, grid, batch]. But our dataset contains the batching dim as the first one, so we need to do some permuting:","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"# Our net wants the input in the form (2,grid,batch), though,\n# So we permute\nxtrain, xtest = permutedims(xtrain,(3,2,1)), permutedims(xtest,(3,2,1)) |> device\nytrain, ytest = permutedims(ytrain,(3,2,1)), permutedims(ytest,(3,2,1)) |> device","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"In order to slice the data into mini-batches, we pass the arrays to the Flux DataLoader.","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"# Pass the data to the Flux DataLoader and give it a batch of 20\ntrain_loader = Flux.Data.DataLoader((xtrain, ytrain), batchsize=20, shuffle=true) |> device\ntest_loader = Flux.Data.DataLoader((xtest, ytest), batchsize=20, shuffle=false) |> device","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"We can now set up the architecture. We lift the inputs to a higher-dimensional space via a simple linear transform using a Dense layer. The input dimensionality is 2, we will transform it to 128. After that, we set up 4 instances of a Fourier Layer where we keep only 16 of the N/2 + 1 = 513 modes that the FFT provides and use the GeLU activation. Finally, we reduce the latent space to the two output arrays we wish to obtain - u1(x) and x:","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"# Set up the Fourier Layer\n# 128 in- and outputs, batch size 20 as given above, grid size 1024\n# 16 modes to keep, σ activation on the gpu\nlayer = FourierLayer(128,128,1024,16,gelu,bias_fourier=false) |> device\n\n# The whole architecture\n# linear transform into the latent space, 4 Fourier Layers,\n# then transform it back\nmodel = Chain(Dense(2,128;bias=false), layer, layer, layer, layer,\n                Dense(128,2;bias=false)) |> device","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"The rest is more or less boilerplate training code for a DNN. We employ the ADAM optimizer with a fixed learning rate of 1e-3, use the mean squared error as loss, evaluate the test loss as callback and train the FNO for 500 epochs.","category":"page"},{"location":"examples/burgers_FNO/","page":"Burgers Equation with FNO","title":"Burgers Equation with FNO","text":"# We use the ADAM optimizer for training\nlearning_rate = 0.001\nopt = ADAM(learning_rate)\n\n# Specify the model parameters\nparameters = params(model)\n\n# The loss function\nloss(x,y) = Flux.Losses.mse(model(x),y)\n\n# Define a callback function that gives some output during training\nevalcb() = @show(loss(xtest,ytest))\n# Print the callback only every 5 seconds, \nthrottled_cb = throttle(evalcb, 5)\n\n# Do the training loop\nFlux.@epochs 500 train!(loss, parameters, train_loader, opt, cb = throttled_cb)","category":"page"},{"location":"reference/","page":"Module Reference","title":"Module Reference","text":"Modules = [OperatorLearning]","category":"page"},{"location":"reference/#OperatorLearning.DeepONet","page":"Module Reference","title":"OperatorLearning.DeepONet","text":"DeepONet(architecture_branch::Tuple, architecture_trunk::Tuple,                         act_branch = identity, act_trunk = identity;                         init_branch = Flux.glorot_uniform,                         init_trunk = Flux.glorot_uniform,                         bias_branch=true, bias_trunk=true) DeepONet(branch_net::Flux.Chain, trunk_net::Flux.Chain)\n\nCreate an (unstacked) DeepONet architecture as proposed by Lu et al. arXiv:1910.03193\n\nThe model works as follows:\n\nx –- branch –                |                 -⊠–u-                | y –- trunk –-\n\nWhere x represents the input function, discretely evaluated at its respective sensors. So the ipnut is of shape [m] for one instance or [m x b] for a training set. y are the probing locations for the operator to be trained. It has shape [N x n] for N different variables in the PDE (i.e. spatial and temporal coordinates) with each n distinct evaluation points. u is the solution of the queried instance of the PDE, given by the specific choice of parameters.\n\nBoth inputs x and y are multiplied together via dot product Σᵢ bᵢⱼ tᵢₖ.\n\nYou can set up this architecture in two ways:\n\nBy Specifying the architecture and all its parameters as given above. This always creates Dense layers for the branch and trunk net and corresponds to the DeepONet proposed by Lu et al.\nBy passing two architectures in the form of two Chain structs directly. Do this if you want more flexibility and e.g. use an RNN or CNN instead of simple Dense layers.\n\nStrictly speaking, DeepONet does not imply either of the branch or trunk net to be a simple DNN. Usually though, this is the case which is why it's treated as the default case here.\n\nExample\n\nConsider a transient 1D advection problem ∂ₜu + u ⋅ ∇u = 0, with an IC u(x,0) = g(x). We are given several (b = 200) instances of the IC, discretized at 50 points each and want to query the solution for 100 different locations and times [0;1].\n\nThat makes the branch input of shape [50 x 200] and the trunk input of shape [2 x 100]. So the input for the branch net is 50 and 100 for the trunk net.\n\nUsage\n\njulia> model = DeepONet((32,64,72), (24,64,72))\nDeepONet with\nbranch net: (Chain(Dense(32, 64), Dense(64, 72)))\nTrunk net: (Chain(Dense(24, 64), Dense(64, 72)))\n\njulia> model = DeepONet((32,64,72), (24,64,72), σ, tanh; init_branch=Flux.glorot_normal, bias_trunk=false)\nDeepONet with\nbranch net: (Chain(Dense(32, 64, σ), Dense(64, 72, σ)))\nTrunk net: (Chain(Dense(24, 64, tanh; bias=false), Dense(64, 72, tanh; bias=false)))\n\njulia> branch = Chain(Dense(2,128),Dense(128,64),Dense(64,72))\nChain(\n  Dense(2, 128),                        # 384 parameters\n  Dense(128, 64),                       # 8_256 parameters\n  Dense(64, 72),                        # 4_680 parameters\n)                   # Total: 6 arrays, 13_320 parameters, 52.406 KiB.\n\njulia> trunk = Chain(Dense(1,24),Dense(24,72))\nChain(\n  Dense(1, 24),                         # 48 parameters\n  Dense(24, 72),                        # 1_800 parameters\n)                   # Total: 4 arrays, 1_848 parameters, 7.469 KiB.\n\njulia> model = DeepONet(branch,trunk)\nDeepONet with\nbranch net: (Chain(Dense(2, 128), Dense(128, 64), Dense(64, 72)))\nTrunk net: (Chain(Dense(1, 24), Dense(24, 72)))\n\n\n\n\n\n","category":"type"},{"location":"reference/#OperatorLearning.FourierLayer","page":"Module Reference","title":"OperatorLearning.FourierLayer","text":"FourierLayer(in, out, grid, modes, σ=identity, init=glorot_uniform) FourierLayer(Wf::AbstractArray, Wl::AbstractArray, [bias_f, bias_l, σ])\n\nCreate a Layer of the Fourier Neural Operator as proposed by Li et al. arXiv: 2010.08895\n\nThe layer does a fourier transform on the grid dimension of the input array, filters higher modes out by the weight matrix and transforms it to the specified output dimension such that In x M x N -> Out x M x N. The output though only contains the relevant Fourier modes with the rest padded to zero in the last axis as a result of the filtering.\n\nThe input x should be a rank 3 tensor of shape (num parameters (in) x num grid points (grid) x batch size (batch)) The output y will be a rank 3 tensor of shape (out x num grid points (grid) x batch size (batch))\n\nYou can specify biases for the paths as you like, though the convolutional path is originally not intended to perform an affine transformation.\n\nExamples\n\nSay you're considering a 1D diffusion problem on a 64 point grid. The input is comprised of the grid points as well as the IC at this point. The data consists of 200 instances of the solution. Beforehand we convert the two input channels into a higher-dimensional latent space with 128 nodes by using a regular Dense layer. So the input takes the dimension 128 x 64 x 200. The output would be the diffused variable at a later time, which initially makes the output of the form 128 x 64 x 200 as well. Finally, we have to squeeze this high-dimensional ouptut into the one quantity of interest again by using a Dense layer.\n\nWe wish to only keep the first 16 modes of the input and work with the classic sigmoid function as activation.\n\nSo we would have:\n\nmodel = FourierLayer(128, 128, 100, 16, σ)\n\n\n\n\n\n","category":"type"},{"location":"reference/#OperatorLearning.cglorot_normal-Tuple{Random.AbstractRNG, Vararg{Any}}","page":"Module Reference","title":"OperatorLearning.cglorot_normal","text":"cglorotnormal([rng=GLOBALRNG], dims...)\n\nA modification of the glorot_normal function provided by Flux to accommodate Complex numbers. This is necessary since the parameters of the global convolution operator in the Fourier Layer generally has complex weights.\n\n\n\n\n\n","category":"method"},{"location":"reference/#OperatorLearning.cglorot_uniform-Tuple{Random.AbstractRNG, Vararg{Any}}","page":"Module Reference","title":"OperatorLearning.cglorot_uniform","text":"cglorotuniform([rng=GLOBALRNG], dims...)\n\nA modification of the glorot_uniform function provided by Flux to accommodate Complex numbers. This is necessary since the parameters of the global convolution operator in the Fourier Layer generally has complex weights.\n\n\n\n\n\n","category":"method"},{"location":"reference/#OperatorLearning.construct_subnet","page":"Module Reference","title":"OperatorLearning.construct_subnet","text":"Construct a Chain of Dense layers from a given tuple of integers.\n\nInput: A tuple (m,n,o,p) of integer type numbers that each describe the width of the i-th Dense layer to Construct\n\nOutput: A Flux Chain with length of the input tuple and individual width given by the tuple elements\n\nExample\n\njulia> model = OperatorLearning.construct_subnet((2,128,64,32,1))\nChain(\n  Dense(2, 128),                        # 384 parameters\n  Dense(128, 64),                       # 8_256 parameters\n  Dense(64, 32),                        # 2_080 parameters\n  Dense(32, 1),                         # 33 parameters\n)                   # Total: 8 arrays, 10_753 parameters, 42.504 KiB.\n\njulia> model([2,1])\n1-element Vector{Float32}:\n -0.7630446\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = OperatorLearning","category":"page"},{"location":"#OperatorLearning","page":"Home","title":"OperatorLearning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Package that provides Layers for the learning of (nonlinear) operators in order to solve parametric PDEs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThis package is still under heavy development and there are likely still a few things to iron out. If you find a bug or something to improve, please feel free to open a new issue or submit a PR in the GitHub Repo","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Simply install by running in a REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add OperatorLearning","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"#Fourier-Neural-Operator","page":"Home","title":"Fourier Neural Operator","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The basic workflow is more or less in line with the layer architectures that Flux provides, i.e. you construct individual layers, chain them if desired and pass the inputs as arguments to the layers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Fourier Layer performs a linear transform as well as convolution (linear transform in fourier space), adds them and passes it through the activation. Additionally, higher Fourier modes are filtered out in the convolution path where you can specify the amount of modes to be kept.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The syntax for a single Fourier Layer is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using OperatorLearning\nusing Flux\n\n# Input = 101, Output = 101, Grid points = 100, Fourier modes = 16\n# Activation: sigmoid (you need to import Flux in your Script to access the activations)\nmodel = FourierLayer(101, 101, 100, 16, σ)\n\n# Same as above, but perform strict convolution in Fourier Space\nmodel = FourierLayer(101, 101, 100, 16, σ; bias_fourier=false)","category":"page"},{"location":"","page":"Home","title":"Home","text":"To see a full implementation, check the corresponding Burgers equation example.","category":"page"},{"location":"#DeepONet","page":"Home","title":"DeepONet","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The workflow here is a little different than with Fourier Neural Operator. In this case, you create the entire architecture by specifying two tuples corresponding to the architecture of branch and trunk net.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This creates a \"vanilla\" DeepONet where branch and trunk net are simply Chains of Dense layers. You can however use any other architecture in the subnets as well, as long as the outputs of the two match. Otherwise, the contraction operation won't work due to dimension mismatch.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using OperatorLearning\nusing Flux\n\n# Create a DeepONet with branch 32 -> 64 -> 72 and sigmoid activation\n# and trunk 24 -> 64 -> 72 and tanh activation without biases\nmodel = DeepONet((32,64,72), (24,64,72), σ, tanh; init_branch=Flux.glorot_normal, bias_trunk=false)\n\n# Alternatively, set up your own nets altogether and pass them to DeepONet\nbranch = Chain(Dense(2,128),Dense(128,64),Dense(64,72))\ntrunk = Chain(Dense(1,24),Dense(24,72))\nmodel = DeepONet(branch,trunk)","category":"page"},{"location":"","page":"Home","title":"Home","text":"To see a full implementation, check the corresponding Burgers equation example.","category":"page"},{"location":"faq/#FAQ","page":"Frequently Asked Questions","title":"FAQ","text":"","category":"section"},{"location":"faq/#What's-the-status-of-this-package?","page":"Frequently Asked Questions","title":"What's the status of this package?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The package as a whole is still under heavy development. However, the layers and models released work well and can be used. See the examples for usage.","category":"page"},{"location":"faq/#What-do-I-need-to-train-an-operator-mapping?-What-are-the-input-data?","page":"Frequently Asked Questions","title":"What do I need to train an operator mapping? What are the input data?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Currently, you need solved instances of the system you're trying to approximate the solution operator of.","category":"page"},{"location":"faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"That is, you'll need to gather data (probably using numerical simulations) that include the solution vector, the grid and the parameters of the PDE (system).","category":"page"},{"location":"faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"In case you want to train a DeepONet, instantiating a grid is trivial since the sensor locations (the grid) does not necessarily need to match the discretization of the input function. So you can just create the arrays yourself as you like.","category":"page"},{"location":"faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"However, future work includes implementing physics-informed operator approximations which have been shown to be able to lighten the amount of training data needed or even alleviate it altogether (see e.g. [1] or [2]).","category":"page"},{"location":"faq/#What-about-hardware-and-distributed-computing?","page":"Frequently Asked Questions","title":"What about hardware and distributed computing?","text":"","category":"section"},{"location":"faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Just like Flux.jl, this package runs nicely on GPUs using CUDA.jl. you can simply pipe your data and function calls using |> gpu using the macro that Flux provides. For usage, see the Burgers equation example. Running on multiple GPUs has however not been tested yet.","category":"page"}]
}
